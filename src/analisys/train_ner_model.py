import pickle
from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer
import csv
import os
import numpy as np
import evaluate
import accelerate

with open(r'.\data\processed\data_ner_an.pkl', 'rb') as f:
    data = pickle.load(f)


tokens = []
ner_tags_bio = []
ids = []

for file_name, file_data in data.items():
    for id_, content in file_data.items():
        if content:
            tokens.append(content["sentences"])
            ner_tags_bio.append(content["labels"])
            ids.append(f"{file_name}_{id_}")

# Definir la codificaci√≥n de etiquetas
label_dict = {'B-AGE': 0, 'B-CONTACT': 1, 'B-DATE': 2, 'B-ID': 3, 'B-LOCATION': 4,
              'B-NAME': 5, 'B-OTHER': 6, 'B-PROFESSION': 7, 'I-AGE': 8, 'I-CONTACT': 9,
              'I-DATE': 10, 'I-ID': 11, 'I-LOCATION': 12, 'I-NAME': 13, 'I-OTHER': 14,
              'I-PROFESSION': 15, 'O': 16}

# Codificar las etiquetas
ner_tags = [[label_dict[label] for label in label_list] for label_list in ner_tags_bio]

# Crear el Dataset
data = {"id": ids, "tokens": tokens, "ner_tags_bio": ner_tags_bio, "ner_tags": ner_tags}
dataset = Dataset.from_dict(data)

# Dividir el dataset en train, validation y test usando train_test_split de datasets
train_test_split = dataset.train_test_split(test_size=0.2, seed=42)
train_val_split = train_test_split['train'].train_test_split(test_size=0.25, seed=42)  # 0.25 * 0.8 = 0.2

# Crear el DatasetDict
raw_datasets = DatasetDict({
    "train": train_val_split['train'],
    "validation": train_val_split['test'],
    "test": train_test_split['test']
})

model_checkpoint = "PlanTL-GOB-ES/roberta-large-bne-capitel-ner"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def align_labels_with_tokens(labels, word_ids):

    """
    Aligns the given labels with the corresponding word IDs from tokenized input.

    This function ensures that the labels are properly aligned with the tokenized words,
    handling special tokens and converting "B-" labels to "I-" labels for subsequent tokens
    of the same word.

    Args:
        labels (List[int]): A list of labels corresponding to each word in the original text.
        word_ids (List[Union[int, None]]): A list of word IDs for each token in the tokenized input.
            `None` values represent special tokens (e.g., padding, [CLS], [SEP]).

    Returns:
        List[int]: A list of aligned labels for each token, with `-100` for special tokens.
    """
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # Special token
            new_labels.append(-100)
        else:
            # Same word as previous token
            label = labels[word_id]
            # If the label is B-XXX we change it to I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels


def tokenize_and_align_labels(examples):
    """
    Tokenizes input text and aligns the corresponding NER labels with the tokenized output.

    This function tokenizes the input text using a tokenizer that splits the text into words 
    and applies truncation. It also aligns the provided NER labels with the tokenized tokens, 
    ensuring that each token is correctly labeled, including handling subword tokens and 
    special tokens appropriately.

    Args:
        examples (Dict[str, List[List[str]]]): A dictionary with two keys:
            - "tokens": A list of lists, where each inner list contains the tokens of a single example.
            - "ner_tags": A list of lists, where each inner list contains the NER labels for the tokens in the corresponding example.

    Returns:
        Dict[str, List]: A dictionary containing the tokenized inputs and the aligned labels, with keys:
            - "input_ids": Token IDs generated by the tokenizer.
            - "attention_mask": Attention mask generated by the tokenizer.
            - "labels": Aligned NER labels corresponding to the tokenized inputs.
    """

    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs

tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)


data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

#Evaluation
metric = evaluate.load("seqeval")

label_names= ['B-AGE', 'B-CONTACT', 'B-DATE', 'B-ID', 'B-LOCATION', 'B-NAME', 'B-OTHER', 'B-PROFESSION',
              'I-AGE', 'I-CONTACT', 'I-DATE', 'I-ID', 'I-LOCATION', 'I-NAME', 'I-OTHER', 'I-PROFESSION', 'O']


def compute_metrics(eval_preds):
    """
    Computes evaluation metrics for NER predictions.

    This function calculates precision, recall, F1-score, and accuracy based on the evaluation predictions 
    and true labels. It removes special tokens and aligns the predictions with the true labels before 
    calculating the metrics.

    Args:
        eval_preds (tuple): A tuple containing:
            - logits (np.ndarray): The predicted logits from the model.
            - labels (np.ndarray): The true labels.

    Returns:
        dict: A dictionary containing the overall precision, recall, F1-score, and accuracy.
    """
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    metrics = {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }

    save_metrics_to_csv(metrics)
    return metrics

def save_metrics_to_csv(metrics, filename=".\results\metrics.csv"):

    """
    Saves evaluation metrics to a CSV file.

    This function creates a directory if it does not exist and writes the provided metrics 
    into a CSV file located in the specified path.

    Args:
        metrics (dict): A dictionary containing the metrics to be saved.
            Expected keys are "precision", "recall", "f1", and "accuracy".
        filename (str, optional): The path where the CSV file will be saved. 
            Defaults to "results/metrics.csv".
    """
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    fieldnames = ["precision", "recall", "f1", "accuracy"]
    
    with open(filename, mode="w", newline='') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerow(metrics)


id2label = {i: label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}


model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)

args = TrainingArguments(
    output_dir=r'.\results',
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()